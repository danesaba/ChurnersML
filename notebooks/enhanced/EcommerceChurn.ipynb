{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOMER CHURN PREDICTION USING MACHINE LEARNING\n",
    "# Bachelor's Thesis - E-Commerce Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook implements customer churn prediction for an e-commerce dataset.\n",
    "\n",
    "### Methodology:\n",
    "1. Data Loading & Exploration\n",
    "2. Preprocessing & Feature Engineering\n",
    "3. Model Training with Cross-Validation\n",
    "4. Hyperparameter Tuning\n",
    "5. Statistical Significance Testing\n",
    "6. Feature Importance Analysis\n",
    "7. Results & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.015151Z",
     "start_time": "2025-12-01T21:41:39.362604Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('Libraries imported successfully')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.464862Z",
     "start_time": "2025-12-01T21:41:40.023388Z"
    }
   },
   "source": [
    "# Load E-Commerce dataset (try Excel first, fall back to CSV)\n",
    "try:\n",
    "    df = pd.read_excel('/Users/nukesaba/PyCharmMiscProject/ECommerceDataset.xlsx')\n",
    "except:\n",
    "    df = pd.read_csv('/Users/nukesaba/PyCharmMiscProject/ECommerceDataset.csv')\n",
    "\n",
    "target = 'Churn'\n",
    "\n",
    "# Drop CustomerID as it's just an identifier\n",
    "if 'CustomerID' in df.columns:\n",
    "    df = df.drop(['CustomerID'], axis=1)\n",
    "\n",
    "print('Dataset Shape:', df.shape)\n",
    "print('\\nFirst 5 rows:')\n",
    "print(df.head())\n",
    "print('\\nTarget Distribution:')\n",
    "print(df[target].value_counts())\n",
    "print(f'\\nImbalance Ratio: {df[target].value_counts()[0] / df[target].value_counts()[1]:.2f}:1')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (5630, 19)\n",
      "\n",
      "First 5 rows:\n",
      "   Churn  Tenure PreferredLoginDevice  CityTier  WarehouseToHome  \\\n",
      "0      1     4.0         Mobile Phone         3              6.0   \n",
      "1      1     NaN                Phone         1              8.0   \n",
      "2      1     NaN                Phone         1             30.0   \n",
      "3      1     0.0                Phone         3             15.0   \n",
      "4      1     0.0                Phone         1             12.0   \n",
      "\n",
      "  PreferredPaymentMode  Gender  HourSpendOnApp  NumberOfDeviceRegistered  \\\n",
      "0           Debit Card  Female             3.0                         3   \n",
      "1                  UPI    Male             3.0                         4   \n",
      "2           Debit Card    Male             2.0                         4   \n",
      "3           Debit Card    Male             2.0                         4   \n",
      "4                   CC    Male             NaN                         3   \n",
      "\n",
      "     PreferedOrderCat  SatisfactionScore MaritalStatus  NumberOfAddress  \\\n",
      "0  Laptop & Accessory                  2        Single                9   \n",
      "1              Mobile                  3        Single                7   \n",
      "2              Mobile                  3        Single                6   \n",
      "3  Laptop & Accessory                  5        Single                8   \n",
      "4              Mobile                  5        Single                3   \n",
      "\n",
      "   Complain  OrderAmountHikeFromlastYear  CouponUsed  OrderCount  \\\n",
      "0         1                         11.0         1.0         1.0   \n",
      "1         1                         15.0         0.0         1.0   \n",
      "2         1                         14.0         0.0         1.0   \n",
      "3         0                         23.0         0.0         1.0   \n",
      "4         0                         11.0         1.0         1.0   \n",
      "\n",
      "   DaySinceLastOrder  CashbackAmount  \n",
      "0                5.0          159.93  \n",
      "1                0.0          120.90  \n",
      "2                3.0          120.28  \n",
      "3                3.0          134.07  \n",
      "4                3.0          129.60  \n",
      "\n",
      "Target Distribution:\n",
      "Churn\n",
      "0    4682\n",
      "1     948\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Imbalance Ratio: 4.94:1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.475673Z",
     "start_time": "2025-12-01T21:41:40.470789Z"
    }
   },
   "source": [
    "print('Missing Values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print('\\nData types:')\n",
    "print(df.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Churn                            0\n",
      "Tenure                         264\n",
      "PreferredLoginDevice             0\n",
      "CityTier                         0\n",
      "WarehouseToHome                251\n",
      "PreferredPaymentMode             0\n",
      "Gender                           0\n",
      "HourSpendOnApp                 255\n",
      "NumberOfDeviceRegistered         0\n",
      "PreferedOrderCat                 0\n",
      "SatisfactionScore                0\n",
      "MaritalStatus                    0\n",
      "NumberOfAddress                  0\n",
      "Complain                         0\n",
      "OrderAmountHikeFromlastYear    265\n",
      "CouponUsed                     256\n",
      "OrderCount                     258\n",
      "DaySinceLastOrder              307\n",
      "CashbackAmount                   0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "Churn                            int64\n",
      "Tenure                         float64\n",
      "PreferredLoginDevice            object\n",
      "CityTier                         int64\n",
      "WarehouseToHome                float64\n",
      "PreferredPaymentMode            object\n",
      "Gender                          object\n",
      "HourSpendOnApp                 float64\n",
      "NumberOfDeviceRegistered         int64\n",
      "PreferedOrderCat                object\n",
      "SatisfactionScore                int64\n",
      "MaritalStatus                   object\n",
      "NumberOfAddress                  int64\n",
      "Complain                         int64\n",
      "OrderAmountHikeFromlastYear    float64\n",
      "CouponUsed                     float64\n",
      "OrderCount                     float64\n",
      "DaySinceLastOrder              float64\n",
      "CashbackAmount                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.486306Z",
     "start_time": "2025-12-01T21:41:40.479530Z"
    }
   },
   "source": [
    "# CRITICAL: Split FIRST to prevent data leakage\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')\n",
    "print(f'\\nClass distribution in training set:')\n",
    "print(y_train.value_counts())\n",
    "print(f'Imbalance ratio: {y_train.value_counts()[0]/y_train.value_counts()[1]:.2f}:1')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4504 samples\n",
      "Test set: 1126 samples\n",
      "\n",
      "Class distribution in training set:\n",
      "Churn\n",
      "0    3746\n",
      "1     758\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 4.94:1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.497129Z",
     "start_time": "2025-12-01T21:41:40.489844Z"
    }
   },
   "source": [
    "# Handle missing values - compute statistics on TRAINING data only\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "# Fill numeric columns with median (training median)\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].isnull().any():\n",
    "        train_median = X_train[col].median()\n",
    "        X_train[col] = X_train[col].fillna(train_median)\n",
    "        X_test[col] = X_test[col].fillna(train_median)\n",
    "\n",
    "# Fill categorical columns with mode (training mode)\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isnull().any():\n",
    "        train_mode = X_train[col].mode()[0]\n",
    "        X_train[col] = X_train[col].fillna(train_mode)\n",
    "        X_test[col] = X_test[col].fillna(train_mode)\n",
    "\n",
    "print('✓ Missing values handled using training set statistics')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Missing values handled using training set statistics\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.511152Z",
     "start_time": "2025-12-01T21:41:40.500958Z"
    }
   },
   "source": [
    "# Encode categorical features - fit on TRAINING data only\n",
    "le_gender = LabelEncoder()\n",
    "le_marital = LabelEncoder()\n",
    "\n",
    "X_train['Gender'] = le_gender.fit_transform(X_train['Gender'])\n",
    "X_test['Gender'] = le_gender.transform(X_test['Gender'])\n",
    "\n",
    "X_train['MaritalStatus'] = le_marital.fit_transform(X_train['MaritalStatus'])\n",
    "X_test['MaritalStatus'] = le_marital.transform(X_test['MaritalStatus'])\n",
    "\n",
    "# One-hot encode multi-category features\n",
    "categorical_cols = ['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat']\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure test set has same columns as train set\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "\n",
    "extra_cols = set(X_test.columns) - set(X_train.columns)\n",
    "for col in extra_cols:\n",
    "    X_test = X_test.drop(columns=[col])\n",
    "\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print('After encoding:')\n",
    "print(X_train.head())\n",
    "print(f'\\nFeature count: {X_train.shape[1]}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoding:\n",
      "      Tenure  CityTier  WarehouseToHome  Gender  HourSpendOnApp  \\\n",
      "1787     9.0         3             16.0       1             2.0   \n",
      "2147     6.0         3             13.0       0             1.0   \n",
      "1717     8.0         1             15.0       1             3.0   \n",
      "2292    15.0         3             11.0       1             3.0   \n",
      "5578    12.0         1             13.0       1             4.0   \n",
      "\n",
      "      NumberOfDeviceRegistered  SatisfactionScore  MaritalStatus  \\\n",
      "1787                         3                  1              2   \n",
      "2147                         3                  4              1   \n",
      "1717                         4                  4              2   \n",
      "2292                         3                  4              2   \n",
      "5578                         5                  3              1   \n",
      "\n",
      "      NumberOfAddress  Complain  ...  PreferredPaymentMode_Cash on Delivery  \\\n",
      "1787                2         0  ...                                  False   \n",
      "2147                1         0  ...                                  False   \n",
      "1717               10         0  ...                                  False   \n",
      "2292               10         1  ...                                  False   \n",
      "5578                4         0  ...                                  False   \n",
      "\n",
      "      PreferredPaymentMode_Credit Card  PreferredPaymentMode_Debit Card  \\\n",
      "1787                              True                            False   \n",
      "2147                             False                             True   \n",
      "1717                             False                             True   \n",
      "2292                             False                             True   \n",
      "5578                             False                             True   \n",
      "\n",
      "      PreferredPaymentMode_E wallet  PreferredPaymentMode_UPI  \\\n",
      "1787                          False                     False   \n",
      "2147                          False                     False   \n",
      "1717                          False                     False   \n",
      "2292                          False                     False   \n",
      "5578                          False                     False   \n",
      "\n",
      "      PreferedOrderCat_Grocery  PreferedOrderCat_Laptop & Accessory  \\\n",
      "1787                     False                                False   \n",
      "2147                     False                                 True   \n",
      "1717                     False                                 True   \n",
      "2292                     False                                False   \n",
      "5578                     False                                 True   \n",
      "\n",
      "      PreferedOrderCat_Mobile  PreferedOrderCat_Mobile Phone  \\\n",
      "1787                    False                          False   \n",
      "2147                    False                          False   \n",
      "1717                    False                          False   \n",
      "2292                    False                          False   \n",
      "5578                    False                          False   \n",
      "\n",
      "      PreferedOrderCat_Others  \n",
      "1787                    False  \n",
      "2147                    False  \n",
      "1717                    False  \n",
      "2292                    False  \n",
      "5578                    False  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "Feature count: 28\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.524502Z",
     "start_time": "2025-12-01T21:41:40.515029Z"
    }
   },
   "source": [
    "# Feature Engineering - parameters computed on TRAINING data only\n",
    "\n",
    "# Cashback efficiency\n",
    "X_train['CashbackEfficiency'] = X_train['CashbackAmount'] / (X_train['OrderAmountHikeFromlastYear'] + 1e-6)\n",
    "X_test['CashbackEfficiency'] = X_test['CashbackAmount'] / (X_test['OrderAmountHikeFromlastYear'] + 1e-6)\n",
    "\n",
    "# Order frequency\n",
    "X_train['OrderFrequency'] = X_train['OrderCount'] / (X_train['Tenure'] + 1)\n",
    "X_test['OrderFrequency'] = X_test['OrderCount'] / (X_test['Tenure'] + 1)\n",
    "\n",
    "# App usage intensity\n",
    "X_train['AppUsageIntensity'] = X_train['HourSpendOnApp'] / (X_train['Tenure'] + 1)\n",
    "X_test['AppUsageIntensity'] = X_test['HourSpendOnApp'] / (X_test['Tenure'] + 1)\n",
    "\n",
    "# High value customer indicator (thresholds from TRAINING data)\n",
    "order_threshold = X_train['OrderCount'].median()\n",
    "cashback_threshold = X_train['CashbackAmount'].median()\n",
    "\n",
    "X_train['IsHighValue'] = (\n",
    "    (X_train['OrderCount'] > order_threshold) |\n",
    "    (X_train['CashbackAmount'] > cashback_threshold)\n",
    ").astype(int)\n",
    "\n",
    "X_test['IsHighValue'] = (\n",
    "    (X_test['OrderCount'] > order_threshold) |\n",
    "    (X_test['CashbackAmount'] > cashback_threshold)\n",
    ").astype(int)\n",
    "\n",
    "# Tenure group binning (quantiles from TRAINING data)\n",
    "q33 = X_train['Tenure'].quantile(0.33)\n",
    "q66 = X_train['Tenure'].quantile(0.66)\n",
    "tenure_bins = [-np.inf, q33, q66, np.inf]\n",
    "\n",
    "X_train['TenureGroup'] = pd.cut(X_train['Tenure'], bins=tenure_bins, labels=[0, 1, 2]).astype(int)\n",
    "X_test['TenureGroup'] = pd.cut(X_test['Tenure'], bins=tenure_bins, labels=[0, 1, 2]).astype(int)\n",
    "\n",
    "print(f'Final feature count: {X_train.shape[1]}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 33\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:41:40.535488Z",
     "start_time": "2025-12-01T21:41:40.528840Z"
    }
   },
   "source": [
    "# Scaling - fit on TRAINING data only\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "print(f'Scaled {len(numeric_cols)} columns')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled 20 columns\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training with Cross-Validation\n",
    "\n",
    "SMOTE is applied INSIDE each CV fold using ImbPipeline to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:42:17.066277Z",
     "start_time": "2025-12-01T21:41:40.538317Z"
    }
   },
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "base_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=RANDOM_STATE, verbose=0),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "print('=' * 60)\n",
    "print('CROSS-VALIDATION (SMOTE inside each fold)')\n",
    "print('=' * 60)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    cv_results[name] = {\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std()\n",
    "    }\n",
    "\n",
    "    print(f'{name:25s}: F1 = {scores.mean():.4f} (+/-{scores.std():.4f})')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROSS-VALIDATION (SMOTE inside each fold)\n",
      "============================================================\n",
      "Logistic Regression      : F1 = 0.6235 (+/-0.0240)\n",
      "Decision Tree            : F1 = 0.7401 (+/-0.0292)\n",
      "k-Nearest Neighbors      : F1 = 0.7274 (+/-0.0114)\n",
      "Naive Bayes              : F1 = 0.4975 (+/-0.0214)\n",
      "Support Vector Machine   : F1 = 0.7557 (+/-0.0300)\n",
      "XGBoost                  : F1 = 0.8660 (+/-0.0153)\n",
      "LightGBM                 : F1 = 0.8354 (+/-0.0240)\n",
      "CatBoost                 : F1 = 0.8368 (+/-0.0179)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:42:20.900806Z",
     "start_time": "2025-12-01T21:42:20.897204Z"
    }
   },
   "source": [
    "cv_summary = pd.DataFrame({\n",
    "    'Model': cv_results.keys(),\n",
    "    'Mean F1': [cv_results[m]['mean'] for m in cv_results],\n",
    "    'Std': [cv_results[m]['std'] for m in cv_results]\n",
    "}).sort_values('Mean F1', ascending=False)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('CV SUMMARY (Sorted by F1-Score)')\n",
    "print('=' * 60)\n",
    "print(cv_summary.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CV SUMMARY (Sorted by F1-Score)\n",
      "============================================================\n",
      "                 Model  Mean F1      Std\n",
      "               XGBoost 0.866036 0.015338\n",
      "              CatBoost 0.836780 0.017920\n",
      "              LightGBM 0.835389 0.024042\n",
      "Support Vector Machine 0.755690 0.029973\n",
      "         Decision Tree 0.740086 0.029173\n",
      "   k-Nearest Neighbors 0.727351 0.011397\n",
      "   Logistic Regression 0.623530 0.023986\n",
      "           Naive Bayes 0.497516 0.021370\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Tuning top 3 models. SMOTE is applied inside CV folds via ImbPipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:42:20.907706Z",
     "start_time": "2025-12-01T21:42:20.905698Z"
    }
   },
   "source": [
    "top_3_models = cv_summary.head(3)['Model'].tolist()\n",
    "print(f'Top 3 models to tune: {top_3_models}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models to tune: ['XGBoost', 'CatBoost', 'LightGBM']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:42:20.927779Z",
     "start_time": "2025-12-01T21:42:20.922546Z"
    }
   },
   "source": [
    "param_grids = {\n",
    "    'LightGBM': {\n",
    "        'classifier__num_leaves': [31, 50, 70],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__min_child_samples': [20, 30, 40]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'classifier__depth': [4, 6, 8],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'classifier__iterations': [100, 200, 300],\n",
    "        'classifier__l2_leaf_reg': [1, 3, 5, 7]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__subsample': [0.8, 0.9, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_instances = {\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=RANDOM_STATE, verbose=0),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:42:20.942581Z",
     "start_time": "2025-12-01T21:42:20.939888Z"
    }
   },
   "source": [
    "def tune_model(model_name, model, param_grid, X_train, y_train, n_iter=50):\n",
    "    \"\"\"Hyperparameter tuning with SMOTE inside CV folds.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f'TUNING: {model_name}')\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        cv=skf,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(f'\\nBest CV F1-Score: {search.best_score_:.4f}')\n",
    "    print('Best Parameters:')\n",
    "    for param, value in search.best_params_.items():\n",
    "        print(f'  {param}: {value}')\n",
    "\n",
    "    return search.best_estimator_, search.best_params_, search.best_score_"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T01:53:13.301685Z",
     "start_time": "2025-12-01T21:42:20.945324Z"
    }
   },
   "source": [
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    if model_name in param_grids:\n",
    "        best_pipeline, best_params, best_score = tune_model(\n",
    "            model_name,\n",
    "            model_instances[model_name],\n",
    "            param_grids[model_name],\n",
    "            X_train, y_train\n",
    "        )\n",
    "        tuned_models[f'{model_name} (Tuned)'] = best_pipeline\n",
    "        tuning_results[model_name] = {\n",
    "            'best_params': best_params,\n",
    "            'cv_score': best_score\n",
    "        }"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TUNING: XGBoost\n",
      "============================================================\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best CV F1-Score: 0.8685\n",
      "Best Parameters:\n",
      "  classifier__subsample: 1.0\n",
      "  classifier__n_estimators: 300\n",
      "  classifier__max_depth: 7\n",
      "  classifier__learning_rate: 0.2\n",
      "  classifier__colsample_bytree: 0.9\n",
      "\n",
      "============================================================\n",
      "TUNING: CatBoost\n",
      "============================================================\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best CV F1-Score: 0.8746\n",
      "Best Parameters:\n",
      "  classifier__num_leaves: 70\n",
      "  classifier__n_estimators: 200\n",
      "  classifier__min_child_samples: 30\n",
      "  classifier__learning_rate: 0.1\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T02:01:20.071745Z",
     "start_time": "2025-12-02T02:01:20.066525Z"
    }
   },
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    return metrics, y_pred"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T02:01:20.864718Z",
     "start_time": "2025-12-02T02:01:20.510290Z"
    }
   },
   "source": [
    "print('=' * 60)\n",
    "print('TEST SET EVALUATION')\n",
    "print('=' * 60)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    metrics, y_pred = evaluate_model(model, X_test, y_test, name)\n",
    "    evaluation_results.append(metrics)\n",
    "\n",
    "    print(f'\\n{name}')\n",
    "    print('-' * 40)\n",
    "    print(f\"Accuracy:  {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['F1-Score']:.4f}\")\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST SET EVALUATION\n",
      "============================================================\n",
      "\n",
      "XGBoost (Tuned)\n",
      "----------------------------------------\n",
      "Accuracy:  0.9938\n",
      "Precision: 0.9893\n",
      "Recall:    0.9737\n",
      "F1-Score:  0.9814\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       936\n",
      "           1       0.99      0.97      0.98       190\n",
      "\n",
      "    accuracy                           0.99      1126\n",
      "   macro avg       0.99      0.99      0.99      1126\n",
      "weighted avg       0.99      0.99      0.99      1126\n",
      "\n",
      "Confusion Matrix:\n",
      "[[934   2]\n",
      " [  5 185]]\n",
      "\n",
      "CatBoost (Tuned)\n",
      "----------------------------------------\n",
      "Accuracy:  0.9911\n",
      "Precision: 0.9839\n",
      "Recall:    0.9632\n",
      "F1-Score:  0.9734\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       936\n",
      "           1       0.98      0.96      0.97       190\n",
      "\n",
      "    accuracy                           0.99      1126\n",
      "   macro avg       0.99      0.98      0.98      1126\n",
      "weighted avg       0.99      0.99      0.99      1126\n",
      "\n",
      "Confusion Matrix:\n",
      "[[933   3]\n",
      " [  7 183]]\n",
      "\n",
      "LightGBM (Tuned)\n",
      "----------------------------------------\n",
      "Accuracy:  0.9902\n",
      "Precision: 0.9891\n",
      "Recall:    0.9526\n",
      "F1-Score:  0.9705\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       936\n",
      "           1       0.99      0.95      0.97       190\n",
      "\n",
      "    accuracy                           0.99      1126\n",
      "   macro avg       0.99      0.98      0.98      1126\n",
      "weighted avg       0.99      0.99      0.99      1126\n",
      "\n",
      "Confusion Matrix:\n",
      "[[934   2]\n",
      " [  9 181]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T02:01:20.903182Z",
     "start_time": "2025-12-02T02:01:20.893748Z"
    }
   },
   "source": [
    "results_df = pd.DataFrame(evaluation_results).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('RESULTS SUMMARY')\n",
    "print('=' * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1-Score']\n",
    "print(f'\\nBest Model: {best_model_name} with F1-Score = {best_f1:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "           Model  Accuracy  Precision   Recall  F1-Score\n",
      " XGBoost (Tuned)  0.993783   0.989305 0.973684  0.981432\n",
      "CatBoost (Tuned)  0.991119   0.983871 0.963158  0.973404\n",
      "LightGBM (Tuned)  0.990231   0.989071 0.952632  0.970509\n",
      "\n",
      "Best Model: XGBoost (Tuned) with F1-Score = 0.9814\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Significance Testing\n",
    "\n",
    "Wilcoxon signed-rank test to compare models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-02T02:01:20.911152Z"
    }
   },
   "source": [
    "print('Collecting CV scores for statistical testing...')\n",
    "\n",
    "cv_scores_dict = {}\n",
    "for name, model in tuned_models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1', n_jobs=-1)\n",
    "    cv_scores_dict[name] = scores\n",
    "    print(f'{name:25s}: {scores.mean():.4f} (+/-{scores.std():.4f})')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting CV scores for statistical testing...\n",
      "XGBoost (Tuned)          : 0.8685 (+/-0.0131)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilcoxon_test(scores_a, scores_b, name_a, name_b):\n",
    "    \"\"\"Wilcoxon signed-rank test between two models.\"\"\"\n",
    "    statistic, p_value = wilcoxon(scores_a, scores_b)\n",
    "\n",
    "    diff = scores_a - scores_b\n",
    "    cohens_d = np.mean(diff) / np.std(diff, ddof=1) if np.std(diff, ddof=1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Model A': name_a,\n",
    "        'Model B': name_b,\n",
    "        'Mean A': np.mean(scores_a),\n",
    "        'Mean B': np.mean(scores_b),\n",
    "        'p-value': p_value,\n",
    "        \"Cohen's d\": cohens_d,\n",
    "        'Significant': p_value < 0.05\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('STATISTICAL SIGNIFICANCE TESTING')\n",
    "print('=' * 60)\n",
    "\n",
    "model_names = list(cv_scores_dict.keys())\n",
    "comparisons = list(combinations(model_names, 2))\n",
    "\n",
    "alpha = 0.05\n",
    "corrected_alpha = alpha / len(comparisons) if len(comparisons) > 0 else alpha\n",
    "print(f'Comparisons: {len(comparisons)}')\n",
    "print(f'Bonferroni-corrected alpha: {corrected_alpha:.6f}')\n",
    "\n",
    "significance_results = []\n",
    "for name_a, name_b in comparisons:\n",
    "    result = wilcoxon_test(\n",
    "        cv_scores_dict[name_a],\n",
    "        cv_scores_dict[name_b],\n",
    "        name_a, name_b\n",
    "    )\n",
    "    result['Significant (Bonferroni)'] = result['p-value'] < corrected_alpha\n",
    "    significance_results.append(result)\n",
    "\n",
    "if len(significance_results) > 0:\n",
    "    significance_df = pd.DataFrame(significance_results)\n",
    "    print('\\n' + significance_df.to_string(index=False))\n",
    "\n",
    "    n_significant = significance_df['Significant'].sum()\n",
    "    n_bonferroni = significance_df['Significant (Bonferroni)'].sum()\n",
    "\n",
    "    print(f'\\nSignificant at alpha=0.05: {n_significant}/{len(comparisons)}')\n",
    "    print(f'Significant with Bonferroni: {n_bonferroni}/{len(comparisons)}')\n",
    "else:\n",
    "    print('\\nNot enough models for pairwise comparisons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Extract feature importance from model.\"\"\"\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        classifier = model.named_steps['classifier']\n",
    "    else:\n",
    "        classifier = model\n",
    "\n",
    "    if hasattr(classifier, 'feature_importances_'):\n",
    "        importance = classifier.feature_importances_\n",
    "        return pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    else:\n",
    "        print(f'{model_name} does not have feature_importances_')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "print('=' * 60)\n",
    "print('FEATURE IMPORTANCE')\n",
    "print('=' * 60)\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    importance_df = get_feature_importance(model, feature_names, name)\n",
    "    if importance_df is not None:\n",
    "        print(f'\\n{name} - Top 10 Features:')\n",
    "        print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuned_models[best_model_name]\n",
    "importance_df = get_feature_importance(best_model, feature_names, best_model_name)\n",
    "\n",
    "if importance_df is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15 = importance_df.head(15)\n",
    "\n",
    "    plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue')\n",
    "    plt.yticks(range(len(top_15)), top_15['Feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Features - {best_model_name}', fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Ecom/feature_importance_ecommerce.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(tuned_models), figsize=(5 * len(tuned_models), 4))\n",
    "if len(tuned_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (name, model) in enumerate(tuned_models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    axes[idx].set_title(f'{name}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Ecom/confusion_matrices_ecommerce.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('FINAL SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print('\\n1. BEST MODEL:')\n",
    "print(f'   {best_model_name}')\n",
    "print(f'   F1-Score: {best_f1:.4f}')\n",
    "print(f\"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "\n",
    "print('\\n2. ALL MODELS:')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print('\\n3. STATISTICAL SIGNIFICANCE:')\n",
    "if len(comparisons) > 0 and len(significance_results) > 0:\n",
    "    if n_bonferroni > 0:\n",
    "        print(f'   {n_bonferroni} significant difference(s) found')\n",
    "    else:\n",
    "        print('   No significant differences - models perform comparably')\n",
    "else:\n",
    "    print('   Not enough models for comparison')\n",
    "\n",
    "print('\\n4. TOP 5 FEATURES:')\n",
    "if importance_df is not None:\n",
    "    for idx, row in importance_df.head(5).iterrows():\n",
    "        print(f\"   {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
